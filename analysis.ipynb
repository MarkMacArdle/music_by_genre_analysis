{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\magar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords') #make sure list up to date\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read in scraped data\n",
    "with open('charts_and_lyrics.json') as json_data:\n",
    "    inputData = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertPOSTag(treebank_tag):\n",
    "    #nltk.pos_tag(word) uses tags from the treebank corpus \n",
    "    #https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "    #but the lemmatiser uses tags from word net so need to convert.\n",
    "    #only noun, verb, adjective and adverb are accepted.\n",
    "    #adapted from https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ #returns 'a'\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB #returns 'v'\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV #returns 'r'\n",
    "\n",
    "    #return noun for both actual nouns and anything that doesn't fit the other three \n",
    "    #noun is the default for the lemmatiser anyway so this will have same effect \n",
    "    #as not passing a tag.\n",
    "    else:\n",
    "        return wordnet.NOUN #returns 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english') #are lowercase\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmaCounts(lyrics, chartName):    \n",
    "    #make sure lyrics were found for this song\n",
    "    #lyrics will be False if none were found\n",
    "    if lyrics:\n",
    "        #strip \"[Chorus]\", \"[Verse 1]\" etc tags\n",
    "        lyrics = re.sub(r'\\[.*\\]', '', lyrics)\n",
    "        \n",
    "        #strip any punctuation\n",
    "        lyrics = re.sub(r'\\W',' ',lyrics)\n",
    "        \n",
    "        #replace any multiple consecutive spaces with just one space\n",
    "        lyrics = re.sub(r'\\s+',' ',lyrics)\n",
    "        \n",
    "        #using .lower both for uniformity and cause stopwords list\n",
    "        #is all lowercase\n",
    "        words = nltk.word_tokenize(lyrics.lower())\n",
    "        \n",
    "        #gives list of tuples in form [(car, NN), (run, VB)... etc]\n",
    "        word_tags = nltk.pos_tag(words)\n",
    "\n",
    "        #add words to word count lists\n",
    "        for wordAndTag in word_tags:\n",
    "            word = wordAndTag[0]\n",
    "            tag = convertPOSTag(wordAndTag[1])\n",
    "            if wordAndTag[0] not in stopwords:\n",
    "                lemma = lemmatizer.lemmatize(word, tag)\n",
    "                lemmaAndTag = (lemma, tag)\n",
    "\n",
    "                #add words to master list\n",
    "                if lemmaAndTag not in allChartsLemmaCounts.keys():\n",
    "                    allChartsLemmaCounts[lemmaAndTag] = {'count': 1, \n",
    "                                                         'sourceWords': [word]}\n",
    "                else:                        \n",
    "                    allChartsLemmaCounts[lemmaAndTag]['count'] = \\\n",
    "                    allChartsLemmaCounts[lemmaAndTag]['count'] + 1\n",
    "                    if word not in allChartsLemmaCounts[lemmaAndTag]['sourceWords']:\n",
    "                        allChartsLemmaCounts[lemmaAndTag]['sourceWords'].append(word)\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "allChartsLemmaCounts = {}\n",
    "\n",
    "for chart in inputData:\n",
    "    for song in chart['entries']:\n",
    "        lemmaCounts(song['lyrics'], chart['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>wordType</th>\n",
       "      <th>count</th>\n",
       "      <th>sourceWords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>get</td>\n",
       "      <td>v</td>\n",
       "      <td>2438</td>\n",
       "      <td>[get, got, getting, gets, gotten]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>know</td>\n",
       "      <td>v</td>\n",
       "      <td>1750</td>\n",
       "      <td>[know, knew, knows, known, knowing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>like</td>\n",
       "      <td>n</td>\n",
       "      <td>1651</td>\n",
       "      <td>[like, likes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yeah</td>\n",
       "      <td>n</td>\n",
       "      <td>1310</td>\n",
       "      <td>[yeah]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>go</td>\n",
       "      <td>v</td>\n",
       "      <td>1070</td>\n",
       "      <td>[go, going, went, gone, goes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>make</td>\n",
       "      <td>v</td>\n",
       "      <td>938</td>\n",
       "      <td>[make, made, making, makes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>na</td>\n",
       "      <td>n</td>\n",
       "      <td>932</td>\n",
       "      <td>[na]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>say</td>\n",
       "      <td>v</td>\n",
       "      <td>914</td>\n",
       "      <td>[say, said, says, saying]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>come</td>\n",
       "      <td>v</td>\n",
       "      <td>899</td>\n",
       "      <td>[come, came, coming, comes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>take</td>\n",
       "      <td>v</td>\n",
       "      <td>840</td>\n",
       "      <td>[take, took, taking, takes, taken]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>oh</td>\n",
       "      <td>n</td>\n",
       "      <td>827</td>\n",
       "      <td>[oh]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>want</td>\n",
       "      <td>v</td>\n",
       "      <td>826</td>\n",
       "      <td>[want, wanted, wants, wanting]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>one</td>\n",
       "      <td>n</td>\n",
       "      <td>737</td>\n",
       "      <td>[one, ones]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>love</td>\n",
       "      <td>n</td>\n",
       "      <td>723</td>\n",
       "      <td>[love, loves]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>feel</td>\n",
       "      <td>v</td>\n",
       "      <td>689</td>\n",
       "      <td>[feeling, feel, feels, feelings]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>time</td>\n",
       "      <td>n</td>\n",
       "      <td>683</td>\n",
       "      <td>[time, times]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>let</td>\n",
       "      <td>v</td>\n",
       "      <td>668</td>\n",
       "      <td>[let, letting, lets]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>see</td>\n",
       "      <td>v</td>\n",
       "      <td>663</td>\n",
       "      <td>[see, seen, seeing, sees]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>never</td>\n",
       "      <td>r</td>\n",
       "      <td>662</td>\n",
       "      <td>[never]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>love</td>\n",
       "      <td>v</td>\n",
       "      <td>652</td>\n",
       "      <td>[love, loved, loving, loves]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>way</td>\n",
       "      <td>n</td>\n",
       "      <td>631</td>\n",
       "      <td>[way, ways]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>back</td>\n",
       "      <td>r</td>\n",
       "      <td>614</td>\n",
       "      <td>[back]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>good</td>\n",
       "      <td>a</td>\n",
       "      <td>568</td>\n",
       "      <td>[good, better]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>bitch</td>\n",
       "      <td>n</td>\n",
       "      <td>560</td>\n",
       "      <td>[bitch, bitches]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>need</td>\n",
       "      <td>v</td>\n",
       "      <td>533</td>\n",
       "      <td>[need, needed, needs, needing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>baby</td>\n",
       "      <td>n</td>\n",
       "      <td>523</td>\n",
       "      <td>[baby, babies]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>heart</td>\n",
       "      <td>n</td>\n",
       "      <td>467</td>\n",
       "      <td>[heart, hearts]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>girl</td>\n",
       "      <td>n</td>\n",
       "      <td>454</td>\n",
       "      <td>[girl, girls]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>tell</td>\n",
       "      <td>v</td>\n",
       "      <td>451</td>\n",
       "      <td>[tell, told, telling, tells]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>oh</td>\n",
       "      <td>a</td>\n",
       "      <td>450</td>\n",
       "      <td>[oh]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8166</th>\n",
       "      <td>lacoste</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>[lacoste]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8167</th>\n",
       "      <td>leaving</td>\n",
       "      <td>n</td>\n",
       "      <td>1</td>\n",
       "      <td>[leaving]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8168</th>\n",
       "      <td>laflare</td>\n",
       "      <td>n</td>\n",
       "      <td>1</td>\n",
       "      <td>[laflare]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8169</th>\n",
       "      <td>laid</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>[laid]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8170</th>\n",
       "      <td>lam</td>\n",
       "      <td>n</td>\n",
       "      <td>1</td>\n",
       "      <td>[lam]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8171</th>\n",
       "      <td>lama</td>\n",
       "      <td>n</td>\n",
       "      <td>1</td>\n",
       "      <td>[lama]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8172</th>\n",
       "      <td>lambo</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>[lambo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173</th>\n",
       "      <td>land</td>\n",
       "      <td>v</td>\n",
       "      <td>1</td>\n",
       "      <td>[landing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8174</th>\n",
       "      <td>landmine</td>\n",
       "      <td>n</td>\n",
       "      <td>1</td>\n",
       "      <td>[landmine]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8175</th>\n",
       "      <td>landslide</td>\n",
       "      <td>n</td>\n",
       "      <td>1</td>\n",
       "      <td>[landslide]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8176</th>\n",
       "      <td>lanez</td>\n",
       "      <td>n</td>\n",
       "      <td>1</td>\n",
       "      <td>[lanez]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8177</th>\n",
       "      <td>lansing</td>\n",
       "      <td>v</td>\n",
       "      <td>1</td>\n",
       "      <td>[lansing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8178</th>\n",
       "      <td>lantern</td>\n",
       "      <td>n</td>\n",
       "      <td>1</td>\n",
       "      <td>[lantern]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8179</th>\n",
       "      <td>lap</td>\n",
       "      <td>n</td>\n",
       "      <td>1</td>\n",
       "      <td>[lap]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8180</th>\n",
       "      <td>las</td>\n",
       "      <td>v</td>\n",
       "      <td>1</td>\n",
       "      <td>[las]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8181</th>\n",
       "      <td>latch</td>\n",
       "      <td>v</td>\n",
       "      <td>1</td>\n",
       "      <td>[latch]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8182</th>\n",
       "      <td>latin</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>[latin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8183</th>\n",
       "      <td>laugh</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>[laugh]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8184</th>\n",
       "      <td>laugh</td>\n",
       "      <td>r</td>\n",
       "      <td>1</td>\n",
       "      <td>[laugh]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8185</th>\n",
       "      <td>lausanne</td>\n",
       "      <td>n</td>\n",
       "      <td>1</td>\n",
       "      <td>[lausanne]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8186</th>\n",
       "      <td>layaway</td>\n",
       "      <td>n</td>\n",
       "      <td>1</td>\n",
       "      <td>[layaway]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8187</th>\n",
       "      <td>layin</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>[layin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8188</th>\n",
       "      <td>lazer</td>\n",
       "      <td>n</td>\n",
       "      <td>1</td>\n",
       "      <td>[lazer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8189</th>\n",
       "      <td>le</td>\n",
       "      <td>v</td>\n",
       "      <td>1</td>\n",
       "      <td>[le]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8190</th>\n",
       "      <td>leaf</td>\n",
       "      <td>n</td>\n",
       "      <td>1</td>\n",
       "      <td>[leaves]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8191</th>\n",
       "      <td>leaky</td>\n",
       "      <td>n</td>\n",
       "      <td>1</td>\n",
       "      <td>[leaky]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8192</th>\n",
       "      <td>leash</td>\n",
       "      <td>n</td>\n",
       "      <td>1</td>\n",
       "      <td>[leash]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8193</th>\n",
       "      <td>leather</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>[leather]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8194</th>\n",
       "      <td>leavin</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>[leavin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8195</th>\n",
       "      <td>ça</td>\n",
       "      <td>n</td>\n",
       "      <td>1</td>\n",
       "      <td>[ça]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8196 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          lemma wordType  count                          sourceWords\n",
       "0           get        v   2438    [get, got, getting, gets, gotten]\n",
       "1          know        v   1750  [know, knew, knows, known, knowing]\n",
       "2          like        n   1651                        [like, likes]\n",
       "3          yeah        n   1310                               [yeah]\n",
       "4            go        v   1070        [go, going, went, gone, goes]\n",
       "5          make        v    938          [make, made, making, makes]\n",
       "6            na        n    932                                 [na]\n",
       "7           say        v    914            [say, said, says, saying]\n",
       "8          come        v    899          [come, came, coming, comes]\n",
       "9          take        v    840   [take, took, taking, takes, taken]\n",
       "10           oh        n    827                                 [oh]\n",
       "11         want        v    826       [want, wanted, wants, wanting]\n",
       "12          one        n    737                          [one, ones]\n",
       "13         love        n    723                        [love, loves]\n",
       "14         feel        v    689     [feeling, feel, feels, feelings]\n",
       "15         time        n    683                        [time, times]\n",
       "16          let        v    668                 [let, letting, lets]\n",
       "17          see        v    663            [see, seen, seeing, sees]\n",
       "18        never        r    662                              [never]\n",
       "19         love        v    652         [love, loved, loving, loves]\n",
       "20          way        n    631                          [way, ways]\n",
       "21         back        r    614                               [back]\n",
       "22         good        a    568                       [good, better]\n",
       "23        bitch        n    560                     [bitch, bitches]\n",
       "24         need        v    533       [need, needed, needs, needing]\n",
       "25         baby        n    523                       [baby, babies]\n",
       "26        heart        n    467                      [heart, hearts]\n",
       "27         girl        n    454                        [girl, girls]\n",
       "28         tell        v    451         [tell, told, telling, tells]\n",
       "29           oh        a    450                                 [oh]\n",
       "...         ...      ...    ...                                  ...\n",
       "8166    lacoste        a      1                            [lacoste]\n",
       "8167    leaving        n      1                            [leaving]\n",
       "8168    laflare        n      1                            [laflare]\n",
       "8169       laid        a      1                               [laid]\n",
       "8170        lam        n      1                                [lam]\n",
       "8171       lama        n      1                               [lama]\n",
       "8172      lambo        a      1                              [lambo]\n",
       "8173       land        v      1                            [landing]\n",
       "8174   landmine        n      1                           [landmine]\n",
       "8175  landslide        n      1                          [landslide]\n",
       "8176      lanez        n      1                              [lanez]\n",
       "8177    lansing        v      1                            [lansing]\n",
       "8178    lantern        n      1                            [lantern]\n",
       "8179        lap        n      1                                [lap]\n",
       "8180        las        v      1                                [las]\n",
       "8181      latch        v      1                              [latch]\n",
       "8182      latin        a      1                              [latin]\n",
       "8183      laugh        a      1                              [laugh]\n",
       "8184      laugh        r      1                              [laugh]\n",
       "8185   lausanne        n      1                           [lausanne]\n",
       "8186    layaway        n      1                            [layaway]\n",
       "8187      layin        a      1                              [layin]\n",
       "8188      lazer        n      1                              [lazer]\n",
       "8189         le        v      1                                 [le]\n",
       "8190       leaf        n      1                             [leaves]\n",
       "8191      leaky        n      1                              [leaky]\n",
       "8192      leash        n      1                              [leash]\n",
       "8193    leather        a      1                            [leather]\n",
       "8194     leavin        a      1                             [leavin]\n",
       "8195         ça        n      1                                 [ça]\n",
       "\n",
       "[8196 rows x 4 columns]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allChartsLemmaCountsDf = pd.DataFrame.from_dict(allChartsLemmaCounts, orient='index')\n",
    "\n",
    "#currently the index of the df is the words and the counts are in a column called 0\n",
    "allChartsLemmaCountsDf.sort_values(by=['count'], ascending=False, inplace=True)\n",
    "\n",
    "#create an actual index and move the words into a column\n",
    "allChartsLemmaCountsDf.reset_index(inplace=True)\n",
    "allChartsLemmaCountsDf.columns = ['lemma', 'wordType', 'count', 'sourceWords']\n",
    "\n",
    "allChartsLemmaCountsDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the longest list of words that was combined by the lemmitisation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(allChartsLemmaCountsDf.sourceWords.apply(len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the lemmitisation of the words is mainly just taking the 's' off the end of some words, eg likes and like. In fact when I check the longest sourceWords list is only 2 long so not many words are being combined. \n",
    "\n",
    "Even just looking at the top 10 there's 'got' is 5th and 'get' is 8th. They should be combined as they're different the present and past tense of the one verb 'get'. \n",
    "\n",
    "Looking into this I've found that this is because the lemmatiser doesn't have the part-of-speech tags for the words so it's assuming nearly everything is a noun."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
